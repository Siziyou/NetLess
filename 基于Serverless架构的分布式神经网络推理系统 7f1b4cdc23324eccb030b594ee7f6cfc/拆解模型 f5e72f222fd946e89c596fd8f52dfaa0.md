# 拆解模型

### 神经网络推理的分布式方案

现有的神经网络分布式方案总体可以分为两个方向：数据分布式与模型分布式。

数据分布式实际上是指在不拆解模型的前提下，将输入数据进行分流，发放给多机，使用模型进行完整的推理后，再收集数据汇总输出——这一过程实际上类似mapreduce的思想，但本质上只是加快了总体推理速度，并没有降低模型硬件门槛。

而模型分布式，则要求将层级的神经网络，拆解形成多个独立的“小模型”，通过增加模型数量来降低单个模型的参数量，在加快模型总体推理速度的同时，有效降低推理的硬件门槛，得益于降低硬件门槛带来的敏捷性这种技术可以和Serverless结合，实现快速、弹性的云服务。

---

### 模型的拆解粒度

模型的拆解粒度是遇到的首个问题，对于多层的神经网络，如果粒度太小，一层一个模型，虽然硬件门槛降低到最低，但是也带来了巨大的通信成本，导致实际模型执行速度的最大限制为网络通信速度，反倒降低了效率；如果粒度太大，那么并没有明显降低硬件门槛，也没有明显的速度提升。

（粒度越粗，沟通成本越低，但是硬件门槛越高）

（粒度越细，硬件门槛越低，但是沟通成本越高）

这里面隐含了一个最优化问题，有没有可能将来对算力负担数据进行数据挖掘，动态拆解模型？（直觉来看应该是一类dp问题，就好像多矩阵乘法中，如何运用乘法结合律安排结合顺序让总计算量最小）

考虑到目前流行的pytorch框架中，常常使用Sequential作为许多层的聚合数据结构，十分方便整体的拆解，因此选定Sequential作为拆解单位。

---

### 实验对象选用resnet系列的原因

经过前期的调研，目前部署在severless上的诸多实验性项目并没有选择将模型的拆解为更细粒度。

如相关论文1，他们并没有改动模型本身的结果，而是在部署测试中，选用了resnet、resnext、squeezenet，这样做的原因是后者的规模更小（mini）。

但是这些模型的结构是不同的，只能在不控制精度、置信度、推理结构等变量的情况下比较延时等指标。

而且在实际程序开发者的需要中，他们并不会随意放弃自己熟用的模型体系，采用简易化的其他模型给工业造成未知性。

因此我选用resnet的原因正是出于上述总结的两点：

1、纵向比较拆解操作对同类模型的影响。

2、保证模型不变的情况下（即推理结果不变），实现对模型推理的性能提升。

---

### Resnet18的拆解

实际上由于编程者的理解方式不同，”层”和“Sequential”之间的含义和组织方式都有差异。以pytorch中预训练好的resnet18模型为例，以下为其详细结构：

```
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)
```

在该版本中，实际上具备四个输出处理层（分别是卷积层、标准化层、激活函数、池化层）紧接着是以layer为变量名的Sequential数据结构，一个Sequential拥有两个基本的残差块（加起来总共8个层），最后一个池化层和线性输出函数收尾，加起来一共18个层，符合原论文的定义，但是可以看到在pytorch中的组织使用了其自身提供的数据结构。

因此我们将前四个层合成一个模型，一个Sequential成为一个模型，最后的两层成为一个模型，也即最后一共6个模型。

```python
import torch.nn as nn
import torchvision.models as models

# 定义 ResNet18 模型
resnet18 = models.resnet18(pretrained=True)

# 将 ResNet18 拆分为 6 个子模型
submodel1 = nn.Sequential(resnet18.conv1, resnet18.bn1, resnet18.relu, resnet18.maxpool)
submodel2 = resnet18.layer1
submodel3 = resnet18.layer2
submodel4 = resnet18.layer3
submodel5 = resnet18.layer4
submodel6 = nn.Sequential(resnet18.avgpool,resnet18.fc)
# 串联 6 个子模型
x = submodel1(inputs)
x = submodel2(x)
x = submodel3(x)
x = submodel4(x)
x = submodel5(x)
return submodel6(x)
```

resnet家族的输出层使用的是线性激活函数，也就意味着，在输出层接受输入之前，张量需要展平。因此实际上上述的过程仅供演示，实际上还需要设计运行结构，来自动添加展平操作。

当然也有另一种拆取办法，通过继承的方法进行处理（以拆两个模型为例）：

```python
import torch.nn as nn
import torchvision.models as models

class ResNetPart1(nn.Module):
    def __init__(self):
        super(ResNetPart1, self).__init__()
        self.conv1 = nn.Sequential(*list(models.resnet18(pretrained=True).children())[:4])

    def forward(self, x):
        x = self.conv1(x)
        return x

class ResNetPart2(nn.Module):
    def __init__(self):
        super(ResNetPart2, self).__init__()
        self.conv1 = nn.Sequential(*list(models.resnet18(pretrained=True).children())[4:-1])
        self.fc = list(models.resnet18(pretrained=True).children())[-1]

    def forward(self, x):
        x = self.conv1(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x
model_part1 = ResNetPart1()
model_part2 = ResNetPart2()
model = nn.Sequential(model_part1, model_part2)
output = model(input)
```

---

### 特殊操作